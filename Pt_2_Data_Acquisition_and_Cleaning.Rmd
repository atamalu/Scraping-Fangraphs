---
title: "FanGraphs Functions Pt.2: Data Acquisition and Cleaning"
output: github_document
---

## Introduction

In part 1, we made a function to construct an url based on your desired parameters. We now have 2 options on how to get this data into R: using the download link or scraping the table manually. Since a download link is not always available, we will go with the latter method.
<BR>
<BR>
Let's start by loading the required functions.
<BR>
```{r}
source('Finished_Functions.R') # function from pt.1 
suppressMessages(library(rvest)) # for scraping
suppressMessages(library(dplyr))

url <- fg_url(Stats = 'bat', Type = 8)
```
<BR>
The "Inspect Element" tool on Firefox and Chrome allows us to get identifiers for different page elements. This is especially useful in cases where the page doesn't render the table as an actual table, or when there are a lot of tables on the page. We can find the css selector of our main table by using the "pick element" tool and clicking on the area of the table wrapper. The table's parts are easy to find; typically labeled as 'th', 'tr', or 'td'.
<BR>
![table id](Images/Table id.PNG)
<BR>
Now we can retrieve the data.
<BR>
```{r}
pg <- read_html(url) # get page
tab.obj <- html_nodes(pg, '#LeaderBoard1_dg1_ctl00') # give table id
main.tab <- html_table(tab.obj, fill = TRUE) # extract table from page 

### There's only 1 table so we can extract the data frame from the list easily
df <- main.tab[[1]]
```
The data frame is returned with a couple rows of non-useful html information. We can filter this out by removing rows that contain the text 'Page size' in the first column (but any column works in this case).
```{r}
df <- df[ grep("Page size", df[,1], invert = TRUE) , ]

head(df, 4)
```

This reveals header names in the first row, so let's make them our data frame's column names then remove that row.

```{r}
colnames(df) <- df[1,]
df <- df[-1,]

head(df, 4)
```
Now we have our table! Since rvest trims leading and trailing whitespaces by default, we have to convert columns with numbers in them from "character" type to "numeric". Additionally, we remove symbols that are difficult to call in R, such as the % or + sign. These symbols are meaningful, so we want to replace them with something that represents them.

```{r}
### Format column names
colnames(df) <- gsub('\\#', 'num', colnames(df))
colnames(df) <- gsub('\\+', 'plus', colnames(df))
colnames(df) <- gsub('\\%', 'perc', colnames(df))
### Format cells
df[4:length(df)] <- sapply(df[4:length(df)], function(x) as.numeric(gsub('[^0-9\\.\\-]', '', x)))
```

The `sapply` command is used to go over each specified column (4 to the end here) and format it as numeric. We add `\\.` and `\\-` at the end of our regex expression to indicate that decimal points and negative signs should be kept during conversion.
<BR>
<BR>

## Functions

Since data you find online is almost always messy, we should make a couple formatting functions to make things faster in the future.
```{r}
### Format numbers in cells
format_nums <- function(x) { as.numeric(gsub('[^0-9\\.\\-]', '', x)) }

### Format column names
format_colnames <- function(x) {
  x <- gsub('\\#', 'num', x)
  x <- gsub('\\+', 'plus', x)
  x <- gsub('\\%', 'perc', x)
  
  return(x)
}
```
Now let's write a function that retrieves a table using any url parameters we might want. In this case, we will use piping from the `dplyr` package to make the code more readable.
```{r}
fg_table <- function(Url){
  
  ### Get table
  df <- Url %>%
    read_html() %>%
    html_nodes('#LeaderBoard1_dg1_ctl00') %>%
    html_table(fill = TRUE)
  df <- df[[1]]
  
  ### Remove page size rows
  df <- df[ grep("Page size", df[,1], invert = TRUE) , ]
  
  ### Get and use column names
  colnames(df) <- df[1,]
  df <- df[-1,]
  
  ### Formatting
  # column names
  colnames(df) <- format_colnames(colnames(df))
  # cells
  df[4:length(df)] <- sapply(df[4:length(df)], format_nums)
  
  return(df)
  
}

head(fg_table(Url = url), 4)

```

`fg_table` can also be used to acquire data for each day a list of dates. If we look over a span of consecutive days, we can get a more detailed idea of player performance over time. When you enter a start and end date, FanGraphs automatically aggregates the data for that time range instead of providing game-by-game statistics. We can gather the data in the latter form by making a vector of dates...

```{r}
day.vec <- seq.Date(from = as.Date('2019-06-01'), 
                       to = as.Date('2019-06-14'),
                       by ='day')
day.vec <- as.character(day.vec) # convert back to character vec
```

...and looping over them as both the start and end date of our functions. This is can be done using the index of each date.

```{r, eval = FALSE}

big.list <- list()
inds <- 1:length(day.vec) # indices

for(i in inds){
  
  url <- fg_url(Stats = 'bat', Type = 8, 
              Qual = '0', 
              Month = 1000, # month = 1000 allows custom date range
              Max.results = 1000,
              Start.date = day.vec[[i]], 
              End.date = day.vec[[i]])
  big.list[[i]] <- fg_table(url)
  
  print(paste('finished with day', i))
  
}

df <- do.call(rbind, big.list) # combine data frames in list

```

Last, let's write our csv to file for use in part 3. We will allow a dynamic input of dates for the file name, so we do not have to type the date range each time. These are easily found using the first and last indices of the date vector.

```{r, eval = FALSE}
file.name <- paste('FG', day.vec[[1]], 'thru', day.vec[[length(day.vec)]], '.csv')

write.csv(df, file.name)
```
